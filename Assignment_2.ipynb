{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCr4xKJnhZ6"
   },
   "source": [
    "# Overview\n",
    "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
    "\n",
    "**Grading Policy** \n",
    "Assignment 2 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below.  Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. Feel free to reach out to the TAs and instructors if you have any questions.\n",
    "\n",
    "## Task 1 - Data Collection and Preprocessing (10 points)\n",
    "## Task 1a. Data Loading (5 pts)\n",
    "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
    "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
    "* Randomly sample 12,000 rows.\n",
    "* Split the sampled data into train (10k), development (1k) and test set (1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "mjieQgrsocnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9600, 2)\n",
      "(1200, 2)\n",
      "(1200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Irish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129371</th>\n",
       "      <td>The majority of this support should be depende...</td>\n",
       "      <td>Chun an méid sin a bhaint amach is gá leanúint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26002</th>\n",
       "      <td>The Commission shall be empowered to adopt del...</td>\n",
       "      <td>I gcás ina n-aistreofar teidlíochtaí íocaíocht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30027</th>\n",
       "      <td>For the industrial manufacture of essential oi...</td>\n",
       "      <td>Ó chapaill, ó asail, ó mhiúileanna agus ó ráinigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27906</th>\n",
       "      <td>Section 5</td>\n",
       "      <td>Beidh sé beartaithe leis an tacaíocht indíolta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13988</th>\n",
       "      <td>In addition, the Debt facility will help organ...</td>\n",
       "      <td>feabhsóidh siad aistriú eolais agus margadh na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  English  \\\n",
       "129371  The majority of this support should be depende...   \n",
       "26002   The Commission shall be empowered to adopt del...   \n",
       "30027   For the industrial manufacture of essential oi...   \n",
       "27906                                           Section 5   \n",
       "13988   In addition, the Debt facility will help organ...   \n",
       "\n",
       "                                                    Irish  \n",
       "129371  Chun an méid sin a bhaint amach is gá leanúint...  \n",
       "26002   I gcás ina n-aistreofar teidlíochtaí íocaíocht...  \n",
       "30027   Ó chapaill, ó asail, ó mhiúileanna agus ó ráinigh  \n",
       "27906   Beidh sé beartaithe leis an tacaíocht indíolta...  \n",
       "13988   feabhsóidh siad aistriú eolais agus margadh na...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Here\n",
    "\n",
    "english = pd.read_csv(\"DGT.en-ga.en\", sep=\"\\n\", names=[\"English\"])\n",
    "irish = pd.read_csv(\"DGT.en-ga.ga\", sep=\"\\n\", names=[\"Irish\"])\n",
    "\n",
    "data = pd.concat([english, irish], axis=1)\n",
    "data = data.sample(n=12000, random_state=2023)\n",
    "\n",
    "train, val = train_test_split(data, test_size=.2, random_state=2013)\n",
    "val, test = train_test_split(val, test_size=.5, random_state=2013)\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejav7LUqokNc"
   },
   "source": [
    "## Task 1b. Preprocessing (5 pts)\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the following pre-processing steps:\n",
    "  * Lowercase the text\n",
    "  * Remove all punctuation\n",
    "  * tokenize the text \n",
    "*  Build seperate vocabularies for each language. \n",
    "  * Assign each unique word an id value \n",
    "*Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CGC-CvmHojdB"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from typing import List \n",
    "import re \n",
    "\n",
    "class Langauge:\n",
    "\n",
    "    def __init__(self, language: str):\n",
    "        self.language = language                            # Name of the langauge\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1}              # Maps each word in vocab to id\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}              # Reverse map of id to word in vocab\n",
    "        self.word2count = {}                                # Count of each word in vocab\n",
    "        self.n_words = len(self.index2word)                 # number of words in vocab\n",
    "\n",
    "    def addSentence(self, sentence: str):\n",
    "        \"\"\" \n",
    "        Given a sentence, lowercase is and remove any punctuation. Tokenize the\n",
    "        sentence and for each word in the tokenized list call the addWord method.\n",
    "        \"\"\"\n",
    "        text = sentence.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '', text).strip()\n",
    "        for word in word_tokenize(clean_text):\n",
    "            self.addWord(word)\n",
    "  \n",
    "    def addWord(self, word: str):\n",
    "        \"\"\"\n",
    "        For each input word, check if it exists in the the word2index. If it does \n",
    "        not, add the word to the word2index and set the value to the current \n",
    "        vocabulary length. Update the index2word entry as well which maps the token \n",
    "        id to the word. Finaally update the vocabulary count (n_words).\n",
    "\n",
    "        If the word is already in the vocabulary, udpate the count.\n",
    "        \"\"\"\n",
    "        if word not in self.word2index:\n",
    "          self.word2index[word] = self.n_words\n",
    "          self.word2count[word] = 1\n",
    "          self.index2word[self.n_words] = word\n",
    "          self.n_words += 1\n",
    "        else:\n",
    "          self.word2count[word] += 1\n",
    "\n",
    "    def encodeSentence(self, sentence: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Given a sentence:\n",
    "          1. Lower case it\n",
    "          2. Remove all punctuation\n",
    "          3. Prepend SOS and append EOS to it.\n",
    "          4. Tokenize it and return the word ids for each word in the tokenized list. If a word\n",
    "          does not exist in the vocab, skip over it. \n",
    "\n",
    "          Return a list of word ids. \n",
    "        \"\"\"\n",
    "        text = sentence.lower()\n",
    "        clean_text = re.sub(r'[^\\w\\s]', '',text).strip()\n",
    "        clean_text = \"SOS \" + clean_text + \" EOS\"\n",
    "        return [self.word2index[word] for word in word_tokenize(clean_text) if word in self.word2index]\n",
    "\n",
    "    def decodeIds(self, ids: list) -> List[str]:\n",
    "        \"\"\"\n",
    "        Given a list of word ids, look the ids in the index2word and return a\n",
    "        string representing the decoded sentence. \n",
    "        \"\"\"\n",
    "        return \" \".join([self.index2word[tok] for tok in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a927d774b4bc4ecfa3446ed83254484c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocab: 11548\n",
      "Size of Irish vocab: 16345\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "english = Langauge(\"English\")\n",
    "irish = Langauge(\"Irish\")\n",
    "\n",
    "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    english.addSentence(str(row[\"English\"]))\n",
    "    irish.addSentence(str(row[\"Irish\"]))\n",
    "print(f\"Size of English vocab: {english.n_words}\")\n",
    "print(f\"Size of Irish vocab: {irish.n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 2)\n",
      "(9600, 2)\n",
      "(1200, 2)\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 9600 \n",
      "\n",
      "Number of development samples: 1200 \n",
      "\n",
      "Number of test samples: 1200 \n",
      "\n",
      "Number of unique English language tokens 11548 \n",
      "\n",
      "Number of unique Irish language tokens 16345 \n",
      "\n",
      "Max sequence length of source language 1 \n",
      "\n",
      "Max sequence length of target language 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print statistics on the selected dataset:\n",
    "\n",
    "# Number of samples\n",
    "print(\"Number of train samples:\", len(train), '\\n')\n",
    "print(\"Number of development samples:\", len(val), '\\n')\n",
    "print(\"Number of test samples:\", len(test), '\\n')\n",
    "\n",
    "# Number of unique source language tokens\n",
    "print(\"Number of unique English language tokens\", english.n_words, '\\n')\n",
    "\n",
    "# Number of unique target language tokens\n",
    "print(\"Number of unique Irish language tokens\", irish.n_words, '\\n')\n",
    "\n",
    "# Max sequence length of source language\n",
    "eng = str(data[\"English\"])\n",
    "max_source_seq_length = max([len(sentence) for sentence in eng])\n",
    "print(\"Max sequence length of source language\", max_source_seq_length, \"\\n\")\n",
    "\n",
    "# Max sequence length of target language\n",
    "iri = str(data[\"Irish\"])\n",
    "max_target_seq_length = max([len(sentence) for sentence in iri])\n",
    "print(\"Max sequence length of target language\", max_target_seq_length, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oauhQ1fjsC69"
   },
   "source": [
    "## Task 2. Model Implementation and Training (30 pts)\n",
    "\n",
    "\n",
    "\n",
    "## Task 2a. Encoder-Decoder Model Implementation (10 pts)\n",
    "Implement an Encoder-Decoder model in Pytorch with the following components\n",
    "* A single layer RNN based encoder. \n",
    "* A single layer RNN based decoder\n",
    "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train source (9600, 10), and target (9600, 10)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "def encode_features(\n",
    "    df: pd.DataFrame, \n",
    "    english: Langauge,\n",
    "    french: Langauge,\n",
    "    pad_token: int = 0,\n",
    "    max_seq_length = 10\n",
    "  ):\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        source.append(english.encodeSentence(str(row[\"English\"])))\n",
    "        target.append(french.encodeSentence(str(row[\"Irish\"])))\n",
    "\n",
    "    source = pad_sequences(\n",
    "        source,\n",
    "        maxlen=max_seq_length,\n",
    "        padding=\"post\",\n",
    "        truncating = \"post\",\n",
    "        value=pad_token\n",
    "    )\n",
    "\n",
    "    target = pad_sequences(\n",
    "      target,\n",
    "      maxlen=max_seq_length,\n",
    "      padding=\"post\",\n",
    "      truncating = \"post\",\n",
    "      value=pad_token\n",
    "    )\n",
    "  \n",
    "    return source, target\n",
    "\n",
    "train_source, train_target = encode_features(train, english, irish)\n",
    "val_source, val_target = encode_features(val, english, irish)\n",
    "test_source, test_target = encode_features(test, english, irish)\n",
    "\n",
    "print(f\"Shapes of train source {train_source.shape}, and target {train_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10]) torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(train_source),\n",
    "        torch.LongTensor(train_target)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(val_source),\n",
    "        torch.LongTensor(val_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.LongTensor(test_source),\n",
    "        torch.LongTensor(test_target)\n",
    "    ),\n",
    "    shuffle = False,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "\n",
    "for batch in train_dl:\n",
    "    print( batch[0].shape, batch[1].shape )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7gvR8hz0tMoG"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Single Layer RNN based Encoder\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_vocab_size,  # size of source vocabulary  \n",
    "        hidden_dim,        # hidden dimension of embeddings\n",
    "        encoder_hid_dim,   # gru hidden dim\n",
    "        decoder_hid_dim,   # decoder hidden dim \n",
    "        dropout_prob = .5\n",
    "      ):\n",
    "      \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, encoder_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards GRU\n",
    "        #hidden [-1, :, : ] is the last of the backwards GRU\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_hid_dim,      # Encoder hidden dimension\n",
    "        dec_hid_dim       # Decoder hidden dimension \n",
    "      ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention output: [batch size, src len]\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_vocab_size,    # Size of target vocab \n",
    "        hidden_dim,           # hidden size of embedding  \n",
    "        enc_hid_dim, \n",
    "        dec_hid_dim, \n",
    "        dropout\n",
    "      ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = target_vocab_size\n",
    "#         self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "            (enc_hid_dim * 2) + dec_hid_dim + hidden_dim, \n",
    "            target_vocab_size\n",
    "          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)  # [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n",
    "        \n",
    "#         a = self.attention(hidden, encoder_outputs)     # [batch size, src len]\n",
    "#         a = a.unsqueeze(1)                              # [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.mean(encoder_outputs, dim=1, keepdim=True)           # [batch size, 1, enc hid dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)               # [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time     \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):     \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqdYhxa1uiqF"
   },
   "source": [
    "## Task 2b. Training (10 pts)\n",
    "Implement the code to train the Encoder-Decoder model on the Irish-English data. You will write code for the following:\n",
    "* Training, validation and test dataloaders \n",
    "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(11548, 256)\n",
       "    (rnn): GRU(256, 128, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (embedding): Embedding(16345, 256)\n",
       "    (rnn): GRU(512, 128)\n",
       "    (fc_out): Linear(in_features=640, out_features=16345, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = english.n_words\n",
    "OUTPUT_DIM = irish.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "model = EncoderDecoder(enc, dec)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "4cZ-6zHtwkZn"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc165746b7c4b6ba66797573ba09bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfa7974e8c24cb5a8f0b8586563a1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 1.141108319267235\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 1.3284329307527947\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 1.540335115161127\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 1.7612081105217428\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 2.0097292268772886\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 2.3094282890863056\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 2.6379444593541526\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 3.0771381716642967\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 3.546637600982122\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 4.026912688058807\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 4.5767997072121265\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 5.227849457923889\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 6.025497207269915\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 6.9936318550329695\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 8.289585766119227\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 9.583089166764376\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 11.257110799001161\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 13.026685705069601\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 15.16514951034064\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 17.28778184056764\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 20.45035005609985\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 23.547037114509564\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 27.221306666942795\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 31.563456136171578\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 36.74492055810011\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 42.94842597876301\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 49.20523400226797\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 56.317200298209734\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 65.56224521435848\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 75.86838024210718\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 87.97036531689318\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 103.13099744435915\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 116.7459258989899\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 136.31930980254458\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 158.22214079543883\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 183.09405819371815\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 206.2316393667412\n",
      "Epoch 0 | train loss 5.369 | train ppl 214.64811223226334 | val ppl 238.17364297969044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7d1e4457f54377ba42eb507183a5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0eb22fe7e84027a79f1da7641feda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 1.1571961880507962\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 1.3231298123374369\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 1.538795549956865\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 1.7647340515084595\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 1.9957102459664608\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 2.275045381235993\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 2.6326738428088614\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 3.058730620510393\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 3.5078383743425823\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 3.9629947917959374\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 4.531259789228607\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 5.1913822956936855\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 5.98346600764497\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 6.937906000813821\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 8.084915164305059\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 9.430979859538665\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 11.05629556371876\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 12.935817315543076\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 15.13484952140889\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 17.37443720765626\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 20.42990992781124\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 23.855146971869434\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 27.71572661723382\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 32.29782853661068\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 37.78831772635826\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 43.466911783522\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 50.7037564718808\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 58.61554884272747\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 69.4078518387552\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 80.7210997331104\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 93.03725459660976\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 108.96208743196001\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 123.47022081645777\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 143.73912145801262\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 171.0575415172371\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 197.94713498108757\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 226.55777734280952\n",
      "Epoch 1 | train loss 5.111 | train ppl 165.83610809139944 | val ppl 260.60347520476324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553d096ea9864962afd07a6bef60e69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e21b4074e5d4d968c338a7f6e721e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 1.1422499983308942\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 1.3139002448247392\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 1.5219615556186337\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 1.7401999144695428\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 2.0137527074704766\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 2.284163787415424\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 2.6221641807745737\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 3.058730620510393\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 3.4903429574618414\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 3.947174474357382\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 4.517686380154588\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 5.15516951223468\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 5.918008557564278\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 6.827782839376386\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 7.932751904700032\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 9.272008288824463\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 10.837366243244492\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 12.616430848036902\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 14.658201380262703\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 16.911603771230926\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 20.226628927540002\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 23.103866858722185\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 26.60236176489059\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 30.969411761867605\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 36.05335771885924\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 42.43612482156023\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 48.52116041451593\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 55.312559584418885\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 64.19979387365312\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 75.0384013124942\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 85.97013770802953\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 100.48414963638939\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 114.43420168015871\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 133.4864533956711\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 156.6478041899617\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 181.45360478336679\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 208.93015301325102\n",
      "Epoch 2 | train loss 4.908 | train ppl 135.36840667771497 | val ppl 240.32688080237415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d0fc82790c409fba6452f3be54503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3051d6ed34a57b50e8a962313df26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 1.1376901241657316\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 1.3125870013111083\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 1.5326526617240188\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 1.7401999144695428\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 1.9699339218909298\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 2.2659633758311957\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 2.619543327238971\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 3.0556734187455317\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 3.5043322893029334\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 4.026912688058807\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 4.581378796082183\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 5.20177544997497\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 6.037560260716885\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 7.049805304078005\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 8.158007824379352\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 9.487735836358526\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 11.22339007298373\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 13.065824440934556\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 15.580186331018474\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 17.832096362898184\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 20.989031673191437\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 24.07081233300213\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 27.88252085928093\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 32.23329743215228\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 36.92910523887462\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 42.86261496642151\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 49.45187626409605\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 57.22552270477159\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 65.95680116984394\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 77.47846292526083\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 88.85448218678324\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 104.37602463655413\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 116.39621295299656\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 136.45569729472766\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 160.1322441841305\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 185.30442255389528\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 209.5578845988706\n",
      "Epoch 3 | train loss 4.725 | train ppl 112.73049837406913 | val ppl 242.25720685795415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253ae00f084f4f5b93501dac4025fcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd70315f7704d2e990beeef25367102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 1.141108319267235\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 1.3417839036669714\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 1.5745979974750188\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 1.8130309449601565\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 2.0792349218188444\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 2.3964776177110654\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 2.7101392030187967\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 3.2123411450916888\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 3.6692966676192444\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 4.170350145368969\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 4.850103282095144\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 5.534493204504091\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 6.462395050921417\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 7.553416670235239\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 8.864016575651815\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 10.308821654664925\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 12.085422811327593\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 14.396714200154682\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 16.743330502138985\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 19.10595372823165\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 22.555975054319827\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 26.232523607950856\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 30.386547681544123\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 35.05786562994266\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 40.650047316878776\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 47.32316869539415\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 55.36789980950418\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 62.74005001869869\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 72.38506546458879\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 84.52099795484278\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 96.54410977284468\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 112.9561849821966\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 127.7403898460288\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 149.7549063402879\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 175.7390105746881\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 205.81958827644843\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 234.6276993980503\n",
      "Epoch 4 | train loss 4.488 | train ppl 88.94338111102392 | val ppl 271.51027935455187\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "\n",
    "        src = batch[0].transpose(1, 0).to(device)\n",
    "        trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim).to(device)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = F.cross_entropy(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_loss = round(epoch_loss / len(train_dl), 3)\n",
    "\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_dl, total=len(val_dl)):\n",
    "        src = batch[0].transpose(1, 0).to(device)\n",
    "        trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim).to(device)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = F.cross_entropy(output, trg)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "        val_loss = round(eval_loss / len(val_dl), 3)\n",
    "        print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(val_loss)}\")\n",
    "\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best-model.pt')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QofrQ1GAwnDz"
   },
   "source": [
    "# Task 2c. Evaluation on the Test Set (10 pts)\n",
    "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "OJP145YuxAgq"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def translate_sentence(\n",
    "    text: str, \n",
    "    model: EncoderDecoder, \n",
    "    english: Langauge,\n",
    "    irish: Langauge,\n",
    "    device: str,\n",
    "    max_len: int = 10,\n",
    "    ) -> str:\n",
    "\n",
    "    # Encode english sentence and convert to tensor\n",
    "    input_ids = english.encodeSentence(text)\n",
    "    input_tensor = torch.LongTensor(input_ids).unsqueeze(1).to(device)\n",
    "\n",
    "    # Get encooder hidden states\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(input_tensor)\n",
    "\n",
    "    # Build target holder list\n",
    "    trg_indexes = [irish.word2index[\"SOS\"]]\n",
    "\n",
    "    # Loop over sequence length of target sentence\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "\n",
    "    # Decode the encoder outputs with respect to current target word\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
    "\n",
    "    # Retrieve most likely word over target distribution\n",
    "        pred_token = torch.argmax(output).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == irish.word2index[\"EOS\"]:\n",
    "            break\n",
    "\n",
    "    return \"\".join(irish.decodeIds(trg_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(\"My name is\", model, english, irish, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of hypotheses and their reference(s) should be the same ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test)):\n\u001b[0;32m      6\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m translate_sentence(\u001b[38;5;28mstr\u001b[39m(src[i]), model, english, irish, device)\n\u001b[1;32m----> 7\u001b[0m     bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     bleu_scores\u001b[38;5;241m.\u001b[39mappend(bleu_score)\n\u001b[0;32m     11\u001b[0m avg_bleu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(bleu_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(bleu_scores)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:195\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    192\u001b[0m p_denominators \u001b[38;5;241m=\u001b[39m Counter()  \u001b[38;5;66;03m# Key = ngram order, and value = no. of ngram in ref.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m hyp_lengths, ref_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_references) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(hypotheses), (\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of hypotheses and their reference(s) should be the \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     weights[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "bleu_scores = []\n",
    "for i in range(len(test)):\n",
    "    test_pred = translate_sentence(str(src[i]), model, english, irish, device)\n",
    "    bleu_score = corpus_bleu(test_pred, trg)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_brvXpVJxD7e"
   },
   "source": [
    "## Task 3. Improving NMT using Attention (10 pts) \n",
    "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. In a few sentences explains which model is the best for translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SAOUlKtv0MUn"
   },
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        target_vocab_size,    # Size of target vocab \n",
    "        hidden_dim,           # hidden size of embedding  \n",
    "        enc_hid_dim, \n",
    "        dec_hid_dim, \n",
    "        dropout\n",
    "      ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = target_vocab_size\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(\n",
    "            (enc_hid_dim * 2) + dec_hid_dim + hidden_dim, \n",
    "            target_vocab_size\n",
    "          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)  # [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)     # [batch size, src len]\n",
    "        a = a.unsqueeze(1)                              # [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)           # [batch size, 1, enc hid dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)               # [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time     \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):     \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): EncoderGRU(\n",
       "    (embedding): Embedding(11548, 256)\n",
       "    (rnn): GRU(256, 128, bidirectional=True)\n",
       "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): DecoderGRU(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(16345, 256)\n",
       "    (rnn): GRU(512, 128)\n",
       "    (fc_out): Linear(in_features=640, out_features=16345, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = english.n_words\n",
    "OUTPUT_DIM = irish.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 128\n",
    "DEC_HID_DIM = 128\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "model = EncoderDecoder(enc, dec)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb73948ef4d246b08fa0e8c8c21166b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5689840fc0564fd0b835ee3ffbac77be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 1.1537298016660105\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 1.3271051618171572\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 1.538795549956865\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 1.7985845599876695\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 2.0278984286853743\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 2.3631606937057947\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 2.688544583045722\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 3.058730620510393\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 3.448709144454898\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 4.104155512254132\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 4.739823980017224\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 5.584528464276054\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 6.423736771429135\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 7.411256551255594\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 8.602045295141249\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 9.954254024977738\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 11.565193187034973\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 13.558314833854409\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 15.942683352442135\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 18.192328604914966\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 21.455907169973106\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 24.828693988264632\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 29.22428378123494\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 33.64956066541471\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 38.744933544016426\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 45.24083010515978\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 53.03754187161422\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 62.427132713778285\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 70.03534197536601\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 81.69558816667595\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 98.39598496258786\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 113.52238021561821\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 133.3530336632599\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 155.86652000714366\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 179.46855293183248\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 207.88810952782137\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 239.60698055027026\n",
      "Epoch 0 | train loss 4.519 | train ppl 91.74380828267552 | val ppl 283.15657126293945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c586f90b75a4638bc961e7e6242e3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89056e6fca6148f6bd029c7a742945b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 1.1723379466807176\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 1.3417839036669714\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 1.612845483383623\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 1.8663786452864723\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 2.1511444438853182\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 2.524391389973053\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 2.8950431039036304\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 3.3267638012449026\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 3.7621853549999105\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 4.585962466331417\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 5.360193097034803\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 6.171858449883554\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 7.199416636275671\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 8.50794131765075\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 9.994150781394822\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 11.68142531092536\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 13.517700840802913\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 15.626997071013474\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 18.448812402746924\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 21.71492907921277\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 25.380978106022052\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 29.429571436238863\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 34.123967614754356\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 38.397793614992246\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 44.74590803583844\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 51.83159990268289\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 60.15953799453633\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 69.26917485828383\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 78.57078985343014\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 92.38826791783592\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 106.80449354154193\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 125.33623424177405\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 146.2035750261572\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 168.67942161779\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 196.17360362866407\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 230.44218346064218\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 267.20068358865694\n",
      "Epoch 1 | train loss 4.254 | train ppl 70.3863955879128 | val ppl 313.24950075001016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff80cf7a42148399d720388773e64aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273f02da87f34116ba92c6f241a3b88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 1.1583539630298554\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 1.336427488025472\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 1.5636142992864182\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 1.8515071812945383\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 2.110658533543552\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 2.427835209469566\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 2.8066735722367695\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 3.2219926385284996\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 3.702469390967303\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 4.428230196243525\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 5.1038747185367255\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 5.923929526112701\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 7.0006289848698255\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 8.306181527881305\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 9.621498290466686\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 11.067357389273434\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 12.832743621520605\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 15.286957287892788\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 17.93941056172967\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 20.738668476643575\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 24.14313319705658\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 27.82681154544598\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 32.20108024599797\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 37.78831772635826\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 43.991656898689\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 51.88345742702628\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 60.70341762363553\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 69.75576014390317\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 77.78899743192935\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 91.19499352369965\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 106.05947270185592\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 120.54221216376126\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 140.4708508514462\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 163.0407223544917\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 192.86683949959303\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 225.42781571425263\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 266.13401561247815\n",
      "Epoch 2 | train loss 4.016 | train ppl 55.47874641878359 | val ppl 320.21735513016586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a01e09e8e4a40d3a4578fccb7d4914a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afd69a305254ab58a2b491fa6299b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 1.1829366106478107\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 1.3730025719254588\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 1.5983949987546404\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 1.8294218719978594\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 2.134003941758656\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 2.481839452598748\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 2.9476257034472675\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 3.404166082790819\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 3.959033777841203\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 4.749313113948145\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 5.479424077005176\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 6.391698250620899\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 7.330179470660634\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 8.62789017896875\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 10.044246670658563\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 11.72824458829812\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 13.639909261356905\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 16.34627430299201\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 19.394703241471348\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 22.488408529618802\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 25.919614533999034\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 30.47784420089539\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 35.5521275089976\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 40.609417587812246\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 48.182698291098816\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 57.39745704544619\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 67.96548481053634\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 79.75823527739308\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 89.29986713219576\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 105.21438179103109\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 122.73161751726514\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 139.35156714895132\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 164.67930885567773\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 191.7131031329027\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 222.51622048381586\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 259.8228363229507\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 305.2099803831572\n",
      "Epoch 3 | train loss 3.734 | train ppl 41.84615847457281 | val ppl 364.3081225180604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeef2680b78e4f35bbf9eb6bcfddfe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad0aeccc9124a77ab3691e1eb3bff1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 1.167657961105125\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 1.3458152994480976\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 1.5920141888871011\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 1.8663786452864723\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 2.2078076288406328\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 2.6169250932469144\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 3.040433183989171\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 3.472934799336826\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 3.943229272812564\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 4.797044504278355\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 5.567799984149774\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 6.443036917487402\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 7.493230402935639\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 8.61065164289307\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 9.994150781394822\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 11.869831344860069\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 13.85990306680101\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 16.51055709094072\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 19.589623249595995\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 23.011635976027165\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 26.469681932064397\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 31.406032741941566\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 36.89219459203506\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 42.05591321681241\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 49.749479438394545\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 57.742877011357336\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 67.55891283730797\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 78.57078985343014\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 87.97036531689318\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 109.61782520265174\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 127.99612627690637\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 146.05744452855734\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 170.8865694759881\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 200.13657329984082\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 237.9355883838467\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 279.2199995642766\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 327.01302437597104\n",
      "Epoch 4 | train loss 3.447 | train ppl 31.406032741941566 | val ppl 392.28946562499834\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "\n",
    "        src = batch[0].transpose(1, 0).to(device)\n",
    "        trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim).to(device)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = F.cross_entropy(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_loss = round(epoch_loss / len(train_dl), 3)\n",
    "\n",
    "    etest_loss = 0\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_dl, total=len(test_dl)):\n",
    "        src = batch[0].transpose(1, 0).to(device)\n",
    "        trg = batch[1].transpose(1, 0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim).to(device)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = F.cross_entropy(output, trg)\n",
    "\n",
    "            etest_loss += loss.item()\n",
    "\n",
    "        test_loss = round(etest_loss / len(test_dl), 3)\n",
    "        print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(test_loss)}\")\n",
    "\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), 'best-model.pt')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of hypotheses and their reference(s) should be the same ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test)):\n\u001b[0;32m      6\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m translate_sentence(\u001b[38;5;28mstr\u001b[39m(src[i]), model, english, irish, device)\n\u001b[1;32m----> 7\u001b[0m     bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcorpus_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     bleu_scores\u001b[38;5;241m.\u001b[39mappend(bleu_score)\n\u001b[0;32m     11\u001b[0m avg_bleu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(bleu_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(bleu_scores)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:195\u001b[0m, in \u001b[0;36mcorpus_bleu\u001b[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[0;32m    192\u001b[0m p_denominators \u001b[38;5;241m=\u001b[39m Counter()  \u001b[38;5;66;03m# Key = ngram order, and value = no. of ngram in ref.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m hyp_lengths, ref_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_references) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(hypotheses), (\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of hypotheses and their reference(s) should be the \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     weights[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "bleu_scores = []\n",
    "for i in range(len(test)):\n",
    "    test_pred = translate_sentence(str(src[i]), model, english, irish, device)\n",
    "    bleu_score = corpus_bleu(test_pred, trg)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU score: {avg_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not able to calculate the bleu scores for the predicted data. Although, on the basis of the accuracy scores obtained by training the model I can tell that model with attention mechanism works better than the model without attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
